{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "\n",
    "from fastchat.model import get_conversation_template\n",
    "import ml_dtypes\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "os.environ[\"TOGETHERAI_API_KEY\"] = \"Your API Key\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your API Key\"\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "from convs import get_conv_attacker, get_conv_target, get_conv_feedbacker, get_conv_optimizer, get_init_msg\n",
    "from utils import load_model_and_tokenizer, get_losses\n",
    "from utils import get_target_responses_API_prop, get_target_responses_local, get_judge_scores_harmbench\n",
    "from strings import gen_string_optimizer, get_feedbacks, get_new_prompts, get_attacks_string_with_timeout\n",
    "from alg import GWW_dfs_min\n",
    "from convs import LLAMA_SYSTEM_MESSAGE\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_path = \"\"\n",
    "\n",
    "model_lam, tokenizer_lam = load_model_and_tokenizer(llama_path,\n",
    "                            low_cpu_mem_usage=True,\n",
    "                            use_cache=False, \n",
    "                            cache_dir=None,\n",
    "                            device = \"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "from io import StringIO\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/centerforaisafety/HarmBench/main/data/behavior_datasets/harmbench_behaviors_text_all.csv'\n",
    "\n",
    "response = requests.get(url)\n",
    "csv_file = StringIO(response.text)\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "df[df['FunctionalCategory'] == 'standard']\n",
    "goals = df['Behavior']\n",
    "goals_id = df['BehaviorID']\n",
    "url = 'https://raw.githubusercontent.com/centerforaisafety/HarmBench/main/data/optimizer_targets/harmbench_targets_text.json'\n",
    "response = requests.get(url)\n",
    "targets= response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attacker_address = \"together_ai/mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "target_address = \"together_ai/meta-llama/Meta-Llama-3-8B-Instruct-Lite\"\n",
    "judge_address = \"gpt-4o-2024-08-06\"\n",
    "\n",
    "attacker_name = \"mixtral\"\n",
    "\n",
    "folder_path = \"\"\n",
    "num_iters = #number of iterations\n",
    "num_branches = #number of branches per reasoning string\n",
    "memory = #Buffer size for the GWW algorithm\n",
    "K = #bucket size for randomization\n",
    "\n",
    "\n",
    "wandb.init(\n",
    "    project=\"Adversarial Reasoning Project\",   # change to your project name\n",
    "    entity=\"\",       # your wandb username or team\n",
    "    config={\n",
    "        \"attacker_address\": attacker_address,\n",
    "        \"target_address\": target_address,\n",
    "        \"judge_address\": judge_address,\n",
    "        \"attacker_name\": \"\",\n",
    "        \"num_iters\": num_iters,        \n",
    "        \"num_branches\": num_branches,  \n",
    "        \"memory\": memory,\n",
    "        \"Buecket\": K,              \n",
    "    }\n",
    ")\n",
    "\n",
    "def log_and_print(msg, goal, step, extra_logs=None):\n",
    "    \"\"\"\n",
    "    Prints a message and logs it to wandb with the given goal id and step.\n",
    "    \n",
    "    Args:\n",
    "        msg (str): The message to log.\n",
    "        goal (int): The current goal id.\n",
    "        step (int): The step number within the current goal.\n",
    "        extra_logs (dict, optional): Additional key-value data to log.\n",
    "    \"\"\"\n",
    "    print(msg)\n",
    "    log_data = {\"goal\": goal, \"step\": step, \"message\": msg}\n",
    "    if extra_logs:\n",
    "        log_data.update(extra_logs)\n",
    "    wandb.log(log_data)\n",
    "    \n",
    "\n",
    "for i in np.random.choice(np.arange(200), 50, replace= False):\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    # Reset the step counter for each new goal\n",
    "    goal_step = 0\n",
    "    \n",
    "    # Initial goal and prompt\n",
    "    prompt = get_init_msg(goals[i], targets[goals_id[i]])\n",
    "    log_and_print(f\"Goal {i}: {goals[i]}\", goal=i, step=goal_step)\n",
    "    goal_step += 1\n",
    "    \n",
    "    prompt_class = GWW_dfs_min(memory)\n",
    "    conv = get_conv_attacker(attacker_name, goals[i], targets[goals_id[i]], prompt)\n",
    "    messages= get_attacks_string_with_timeout(attacker_address, conv, 16)\n",
    "    losses, _ = get_losses(model_lam, tokenizer_lam, messages, targets[goals_id[i]], \"mistral\") #name of the model changes the way we read the logits here. Be careful! \n",
    "    prompt_class.add_prompt(prompt, losses, messages)\n",
    "\n",
    "    iter = 0\n",
    "    \n",
    "    log_and_print(f\"{'='*36}\\nIteration: {iter}\\n{'='*36}\", goal=i, step=goal_step)\n",
    "    goal_step += 1\n",
    "\n",
    "    init_prompt, losses, messages = prompt_class.get_prompt()\n",
    "    log_and_print(\"Current initial prompt: \" + init_prompt, goal=i, step=goal_step)\n",
    "    goal_step += 1\n",
    "\n",
    "    loss_mean = torch.mean(losses).item()\n",
    "    loss_min = torch.min(losses).item()\n",
    "    log_and_print(\n",
    "        f\"loss = {loss_mean}, min_loss = {loss_min}\",\n",
    "        goal=i,\n",
    "        step=goal_step,\n",
    "        extra_logs={\"loss_mean\": loss_mean, \"loss_min\": loss_min}\n",
    "    )\n",
    "    goal_step += 1\n",
    "    \n",
    "    idx = losses.argsort()\n",
    "    best_prompt = messages[idx[0]]\n",
    "    log_and_print(\"Best jailbraking prompt: \" + best_prompt, goal=i, step=goal_step)\n",
    "    goal_step += 1\n",
    "\n",
    "\n",
    "    # outputs = get_target_responses_API(target_address, messages)\n",
    "    outputs = get_target_responses_local(model_lam, tokenizer_lam, messages)\n",
    "    scores = get_judge_scores_harmbench(judge_address, goals[i], outputs)\n",
    "   \n",
    "    if 1 in scores:\n",
    "        for id, score in enumerate(scores):\n",
    "            if score == 1:\n",
    "                log_and_print(\"The jailbreaking prompt: \" + messages[id], goal=i, step=goal_step)\n",
    "                goal_step += 1\n",
    "                log_and_print(\"Target's response: \" + outputs[id], goal=i, step=goal_step)\n",
    "                goal_step += 1\n",
    "        log_and_print(\"Found a jailbreak. Exiting.\", goal=i, step=goal_step)\n",
    "        goal_step += 1\n",
    "        continue\n",
    "\n",
    "    # Iterative refinement loop for the current goal\n",
    "    for iter in range(1, num_iters):\n",
    "        final_feedbacks = get_feedbacks(attacker_name, attacker_address, goals[i], targets[goals_id[i]], messages, idx, K, num_branches)\n",
    "        print(\"Finished extracting the feedbacks!\")\n",
    "        \n",
    "        collections_opt = [gen_string_optimizer(init_prompt, final_feedback) for final_feedback in final_feedbacks]\n",
    "        convs_opt = [get_conv_optimizer(attacker_name, goals[i], targets[goals_id[i]], collection_opt) for collection_opt in collections_opt]\n",
    "        new_prompts = get_new_prompts(convs_opt, attacker_address)\n",
    "        print(\"Finished extracting the new prompts!\")\n",
    "        \n",
    "        for ext, prompt in enumerate(new_prompts):\n",
    "            conv = get_conv_attacker(attacker_name, goals[i], targets[goals_id[i]], prompt)\n",
    "            messages = get_attacks_string_with_timeout(attacker_address, conv, 16)\n",
    "            losses, _ = get_losses(model_lam, tokenizer_lam, messages, targets[goals_id[i]], \"mistral\")\n",
    "            \n",
    "            prompt_class.add_prompt(prompt, losses, messages)\n",
    "            ext_loss = torch.min(losses).item()\n",
    "            log_and_print(\n",
    "                f\"Extension {ext}'s Loss: {ext_loss}\",\n",
    "                goal=i,\n",
    "                step=goal_step,\n",
    "                extra_logs={f\"extension_{ext}_loss\": ext_loss}\n",
    "            )\n",
    "            goal_step += 1\n",
    "            \n",
    "        log_and_print(f\"{'='*36}\\nIteration: {iter}\\n{'='*36}\", goal=i, step=goal_step)\n",
    "        goal_step += 1\n",
    "\n",
    "        init_prompt, losses, messages = prompt_class.get_prompt()\n",
    "        log_and_print(\"Current initial prompt: \" + init_prompt, goal=i, step=goal_step)\n",
    "        goal_step += 1\n",
    "             \n",
    "        loss_mean = torch.mean(losses).item()\n",
    "        loss_min = torch.min(losses).item()\n",
    "        log_and_print(\n",
    "            f\"loss = {loss_mean}, min_loss = {loss_min}\",\n",
    "            goal=i,\n",
    "            step=goal_step,\n",
    "            extra_logs={\"loss_mean\": loss_mean, \"loss_min\": loss_min}\n",
    "        )\n",
    "        goal_step += 1\n",
    "\n",
    "        idx = losses.argsort()\n",
    "        best_prompt = messages[idx[0]]\n",
    "        log_and_print(\"Best jailbraking prompt: \" + best_prompt, goal=i, step=goal_step)\n",
    "        goal_step += 1\n",
    "                \n",
    "        # outputs = get_target_responses_API(target_address, messages)\n",
    "        outputs = get_target_responses_local(model_lam, tokenizer_lam, messages)\n",
    "        scores = get_judge_scores_harmbench(judge_address, goals[i], outputs)\n",
    "\n",
    "        if 1 in scores:\n",
    "            for id, score in enumerate(scores):\n",
    "                if score == 1:\n",
    "                    log_and_print(\"The jailbraking prompt: \" + messages[id], goal=i, step=goal_step)\n",
    "                    goal_step += 1\n",
    "                    log_and_print(\"Target's response: \" + outputs[id], goal=i, step=goal_step)\n",
    "                    goal_step += 1\n",
    "            log_and_print(\"Found a jailbreak. Exiting.\", goal=i, step=goal_step)\n",
    "            goal_step += 1\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
